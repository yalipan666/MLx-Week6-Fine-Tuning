{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e008ccfe-ef29-491b-b7b0-cc0d6b7f0966",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///notebooks\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers==4.51.0.dev0)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.51.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.0.dev0)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.51.0.dev0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.51.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.51.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.51.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.51.0.dev0) (2020.6.20)\n",
      "Downloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building editable for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.51.0.dev0-0.editable-py3-none-any.whl size=15196 sha256=5062a0264d318a24db63181c8648b3912c999885d3048e7a4b8d683d8ab0bb4a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-f3h16e0j/wheels/92/b1/e0/ce84c2fbc0ed2bc7398e47cccef44e81a78f9916c16dd6e511\n",
      "Successfully built transformers\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed huggingface-hub-0.30.1 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/CarperAI/trlx.git\n",
    "\n",
    "!cd trlx\n",
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu116 # for cuda\n",
    "!pip install -e . \n",
    "! pip install 'accelerate>=0.26.0'\n",
    "! pip install --upgrade bitsandbytes --no-cache-dir\n",
    "! pip install -U peft\n",
    "! pip install evaluate\n",
    "! pip install rouge_score\n",
    "! pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59833237-6503-4a42-8905-839979cf84e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length=550):\n",
    "        self.post_list = []\n",
    "        dataset = load_dataset(train_path, split=split)\n",
    "        for sample in dataset:\n",
    "            self.post_list.append(sample[\"prompt\"] + sample[\"label\"])\n",
    "        if \"valid\" in split:\n",
    "            self.post_list = self.post_list[0:2000]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.post_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.post_list[idx]\n",
    "        encodings_dict = self.tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
    "        attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_masks,\n",
    "            \"labels\": input_ids,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d01ff5d-1a40-4f8a-9604-efaf1a8275d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 00:38:23.337048: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-06 00:38:23.337101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-06 00:38:23.338178: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 00:38:23.344108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-06 00:38:24.051416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "# from summarize_dataset import TLDRDataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d80b6f-9bfa-42f6-8f86-097cbd53c383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89ea54c-abe7-4724-9131-4c02eac0ae4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = \"/notebooks/gpt2-supervised-summarize-checkpoint\"\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "eval_batch_size = 1\n",
    "eval_steps = 500\n",
    "max_input_length = 550\n",
    "save_steps = 1000\n",
    "num_train_epochs = 5\n",
    "random.seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", use_cache=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c756ed0-c398-4f19-b1c3-cb8c53bb20bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b54e15c5f1349f89b4013145e57136c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/532 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9eab9171eb4616a0b32e55d8facef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-e8c59e5cf7bce1c0.parquet:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cac59389af94481bcdea3ca8639d2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-59ffb27399371eac.parquet:   0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54c3acc25a744acbfbaf3608abb53e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-0e33e6bd86e3edc9.parquet:   0%|          | 0.00/6.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4f098046bc4fc2976555d9fed74e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/116722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707b70088cba48999caf386556746522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6bb8ad4cd8421a908acc00b8e02eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/6447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"CarperAI/openai_summarize_tldr\"\n",
    "train_dataset = TLDRDataset(\n",
    "    data_path,\n",
    "    tokenizer,\n",
    "    \"train\",\n",
    "    max_length=max_input_length,\n",
    ")\n",
    "dev_dataset = TLDRDataset(\n",
    "    data_path,\n",
    "    tokenizer,\n",
    "    \"valid\",\n",
    "    max_length=max_input_length,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f691649-8f13-47f1-884c-e84cfeeec846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(data_path, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b48205-0e35-4061-a891-dacf1fafa07b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'label'],\n",
       "    num_rows: 116722\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c165089-98dd-4f41-8e5c-256f7e7bb4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    return result\n",
    "\n",
    "# Create a preprocessing function to extract out the proper logits from the model output\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db26abf-2e67-4217-aa04-ce1928a333bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/src/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the trainer and start training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_accumulation_steps=1,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_checkpointing=True,\n",
    "    half_precision_backend=True,\n",
    "    fp16=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.95,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_steps=100,\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    max_steps=29000,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    # deepspeed=\"/notebooks/ds_config_gptj.json\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6a76c5c-2236-42ac-85d0-b54c796e661d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f6a9807a610>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f6b01212d50, raw_cell=\"trainer.train(resume_from_checkpoint=\"/notebooks/g..\" store_history=True silent=False shell_futures=True cell_id=a6a76c5c-2236-42ac-85d0-b54c796e661d>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29000' max='29000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29000/29000 10:59, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>1.897600</td>\n",
       "      <td>1.860059</td>\n",
       "      <td>0.593090</td>\n",
       "      <td>0.190521</td>\n",
       "      <td>0.393006</td>\n",
       "      <td>0.516792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.889400</td>\n",
       "      <td>1.860014</td>\n",
       "      <td>0.592969</td>\n",
       "      <td>0.190446</td>\n",
       "      <td>0.393017</td>\n",
       "      <td>0.516693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f6a9807a610>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f6b01211590, execution_count=23 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f6b01212d50, raw_cell=\"trainer.train(resume_from_checkpoint=\"/notebooks/g..\" store_history=True silent=False shell_futures=True cell_id=a6a76c5c-2236-42ac-85d0-b54c796e661d> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa152f01-ee24-4b5f-993e-b610824836b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8085625-0851-4cb3-a5e4-889b7fa7f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model to pytorch.bin\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/notebooks/gpt2-supervised-summarize-checkpoint/\").to(\"cuda\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "model.save_pretrained(\"/notebooks/gpt2-supervised-summarize-checkpoint/\", safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcba10b3-da88-47b4-a7a5-dac4a92a3cfd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f6a9807a610>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f6bf8cc7350, raw_cell=\"import wandb\n",
      "wandb.finish()\" store_history=True silent=False shell_futures=True cell_id=bcba10b3-da88-47b4-a7a5-dac4a92a3cfd>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/rouge1</td><td>█▁</td></tr><tr><td>eval/rouge2</td><td>█▁</td></tr><tr><td>eval/rougeL</td><td>▁█</td></tr><tr><td>eval/rougeLsum</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁███████████████████████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁███████████████████████</td></tr><tr><td>train/grad_norm</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▄███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.86001</td></tr><tr><td>eval/rouge1</td><td>0.59297</td></tr><tr><td>eval/rouge2</td><td>0.19045</td></tr><tr><td>eval/rougeL</td><td>0.39302</td></tr><tr><td>eval/rougeLsum</td><td>0.51669</td></tr><tr><td>eval/runtime</td><td>136.6279</td></tr><tr><td>eval/samples_per_second</td><td>14.638</td></tr><tr><td>eval/steps_per_second</td><td>14.638</td></tr><tr><td>total_flos</td><td>1.304460153216e+17</td></tr><tr><td>train/epoch</td><td>3.97478</td></tr><tr><td>train/global_step</td><td>29000</td></tr><tr><td>train/grad_norm</td><td>0.61213</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.8894</td></tr><tr><td>train_loss</td><td>0.06516</td></tr><tr><td>train_runtime</td><td>660.3287</td></tr><tr><td>train_samples_per_second</td><td>702.68</td></tr><tr><td>train_steps_per_second</td><td>43.918</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">/notebooks/gptj-supervised-summarize-checkpoint</strong> at: <a href='https://wandb.ai/kookyghost-university-of-connecticut/huggingface/runs/pjgl14cx' target=\"_blank\">https://wandb.ai/kookyghost-university-of-connecticut/huggingface/runs/pjgl14cx</a><br/> View job at <a href='https://wandb.ai/kookyghost-university-of-connecticut/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjU5NDcwMzUwOQ==/version_details/v1' target=\"_blank\">https://wandb.ai/kookyghost-university-of-connecticut/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjU5NDcwMzUwOQ==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250401_033125-pjgl14cx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2373ffd2-7b31-43d7-96a5-0981c5c49dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dd67be11f64c14917755169dece236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/532 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e64a3e3d754b8bb6933a81a98bd2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4505a5074bd24f918375c861c9b9c696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eca259c85934c0ab7362918fca08879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae3c7b2b2e4427b96ad5ea5ff4371ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389e6d888af44400acce3138aed82349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dde5bf314be43a092b7c1f60c593ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/116722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a861b81e59042528ca66d4f1bf7af0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019cfe53a0f248769523296aecfa48c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/6447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"SUBREDDIT: r/relationships\\nTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\\nPOST: Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\\nTL;DR: \", 'label': \"I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_tldr\", split=\"train\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4283291c-7b54-44bd-9d5a-ac69983f43e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8ec6f5-945e-4ef9-b650-fa1006e97edf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9e0fe32f2e4272a940ecb4ff22d7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cf4a04a4134e98bf11ce8cb9dc191c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81ac33e51084525b9daa1ef84def03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac338ecd46cd4247a7dfbb2cfaac84a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16a97f967a84155a3fc8e37d2473a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 19:24:18.075346: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-06 19:24:18.075411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-06 19:24:18.076596: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 19:24:18.083371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-06 19:24:18.814775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): lora.Linear(\n",
       "            (base_layer): Conv1D(nf=2304, nx=768)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): lora.Linear(\n",
       "            (base_layer): Conv1D(nf=3072, nx=768)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D(nf=768, nx=3072)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/notebooks/gpt2-supervised-summarize-checkpoint/checkpoint-29000\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d8e5c7-5ca5-4bfc-991d-5fcb8859372a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2cf663-c794-442c-8ab4-f8b9f095dbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_generate(batch):\n",
    "    inputs = tokenizer(\n",
    "        batch[\"prompt\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    summaries = [text.split(\"TL;DR:\")[-1].strip() for text in decoded]\n",
    "    return {\"generated\": summaries}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f316b2b-425c-491b-8f30-e70fe41dcc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarperAI/openai_summarize_tldr\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[43mbatch_generate\u001b[49m, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_generate' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_tldr\", split=\"test\")\n",
    "dataset = dataset.map(batch_generate, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd963894-65f0-488a-8e15-4d1cac46c913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'label'],\n",
       "    num_rows: 6553\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_tldr\", split=\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07456aee-fa45-4bec-a230-ab1ce31e7db4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.3156\n",
      "rouge2: 0.1086\n",
      "rougeL: 0.2436\n",
      "rougeLsum: 0.2435\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = dataset[\"generated\"]\n",
    "references = [ref.strip() for ref in dataset[\"label\"]]  # or \"completion\"\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4cf8e3-b5ae-44ad-b184-7a83d8eb8557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:10:41.148499: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-02 19:10:41.148558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-02 19:10:41.151981: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-02 19:10:41.172595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-02 19:10:42.551174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346a271898bc487da98238c340e18b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.0001\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0001\n",
      "rougeLsum: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = dataset[\"generated\"]\n",
    "references = [ref.strip() for ref in dataset[\"label\"]]  # or \"completion\"\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c2a172-2437-4d5b-9e78-c2cc255275b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain gravitycludedcludedcludedcludedcludedcludedcludedcludedcludedcludedveveveveveveve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(tokenizer(\"Explain gravity\", return_tensors=\"pt\").input_ids.to(model.device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "343f1a8b-e02e-4aeb-ba3e-e8d93afa14e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.7.1\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peft==0.7.1) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (2.1.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (4.51.0.dev0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (1.6.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (0.30.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.7.1) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.7.1) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.7.1) (1.3.0)\n",
      "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "Successfully installed peft-0.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install peft==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621be59a-fd6b-4314-a43b-6efd9b6193d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d9dcde0-3454-4fc0-b8d3-f9967eb18a23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO: SUBREDDIT: r/relationships\n",
      "TITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\n",
      "POST: Hello hello everybody. I hope this isn't too trivial of a question to ask on here, but I've been feeling a bit out of my depth when it comes to this situation (I've had only one relationship before, and for many reasons, it was out of the ordinary).\n",
      "\n",
      "Okay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We're both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\n",
      "\n",
      "Okay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \n",
      "\n",
      "We ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\n",
      "\n",
      "Yesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I'm wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I've been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n",
      "TL;DR:  I'm not interested in having a fuck buddy. I'm not interested in having sex with him. I'm not interested in having sex with him. I'm not interested in having sex with him. I'm not interested in having sex with him\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load LoRA adapter checkpoint\n",
    "dpo_model2 = PeftModel.from_pretrained(base_model, \"/notebooks/dpo_gpt2_lora_out_openai_data/checkpoint-34701\")\n",
    "dpo_model2.to(device)\n",
    "\n",
    "# Tokenize input and move to the correct device\n",
    "input_text = dataset[0]['prompt']\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate output\n",
    "dpo_output = dpo_model2.generate(input_ids, max_new_tokens=50)\n",
    "\n",
    "# Decode and print the result\n",
    "print(\"DPO:\", tokenizer.decode(dpo_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c857b41d-b1da-4b33-bcc3-a6b663889bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUBREDDIT: r/relationships\\nTITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\\nPOST: Hello hello everybody. I hope this isn\\'t too trivial of a question to ask on here, but I\\'ve been feeling a bit out of my depth when it comes to this situation (I\\'ve had only one relationship before, and for many reasons, it was out of the ordinary).\\n\\nOkay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We\\'re both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\\n\\nOkay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \\n\\nWe ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\\n\\nYesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I\\'m wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I\\'ve been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I\\'m worried that I may have ruined my chances of a relationship by sleeping with him already.\\nTL;DR: '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10de0fc-be07-4d32-b13d-8f2c3de677a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42164637-98fa-4761-baf4-db840b27ce1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO: SUBREDDIT: r/relationships\n",
      "TITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\n",
      "POST: Hello hello everybody. I hope this isn't too trivial of a question to ask on here, but I've been feeling a bit out of my depth when it comes to this situation (I've had only one relationship before, and for many reasons, it was out of the ordinary).\n",
      "\n",
      "Okay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We're both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\n",
      "\n",
      "Okay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \n",
      "\n",
      "We ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\n",
      "\n",
      "Yesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I'm wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I've been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n",
      "TL;DR:  I slept with a guy on Facebook, and he said he wanted to \"play it by ear\" and not slap any labels on anything. I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the fine-tuned GPT-2 model checkpoint (not a LoRA adapter)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/notebooks/dpo-gpt2-summarize/checkpoint-500\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize input and move to the correct device\n",
    "input_text = dataset[0]['prompt']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print the result\n",
    "print(\"DPO:\", tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752158bc-5cd2-433c-8737-900a753405bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_SDP_ATTENTION\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8344f02-1519-4ebb-9be1-1cfbdb761efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2-1.5B-Instruct\"  # <- no LoRA pre-applied\n",
    "adapter_path = \"dpo-gpt2-summarize/checkpoint-500/qwen-dpo-checkpoint/checkpoint-80\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8f86869-a2f7-41ee-87ab-af7676433a02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SUBREDDIT: r/relationships\\nTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\\nPOST: Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\\nTL;DR: \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59503181-9658-4b4a-a63d-6c360c3a0f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original content without TL;DR:\n",
    "content = dataset[0]['prompt'].strip()\n",
    "content = re.sub(r\"TL;DR:\\s*$\", \"\", content)\n",
    "\n",
    "# Wrap in chat format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that summarizes Reddit posts.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following post:\\n{content}\"}\n",
    "]\n",
    "\n",
    "# Tokenize using chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ce60216-1693-4851-8492-73ff820b2a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/116722 [00:56<203:39:46,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚨 Problem found at index 9\n",
      "Prompt:\n",
      " <|im_start|>system\n",
      "You are a helpful AI assistant that summarizes Reddit posts.<|im_end|>\n",
      "<|im_start|>user\n",
      "Summarize the following post:\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: Me [20/F] having trouble with boyfriend [23/m] of 2.5 years sex drive\n",
      "POST: My boyfriend and I have an amazing sex life. We are very sexually compatible.\n",
      "\n",
      "Although, his sex drive is crazy high compared to mine. It really depends on the time of month for me [hormones], sometimes I'll be down to go 3 times a day and sometimes I'm only interested in once a week. \n",
      "\n",
      "I'm asking for advice because I have a hard time denying him without him feeling rejected by me. For example, every time we cuddle he is almost always coming on to me. At night he comes on to me, in the middle of the night he masturbates next to me, and tries to come onto me in the morning. I'm utterly flattered that after being in a relationship this long he is still very turned on by me, but sometimes it is very overwhelming to keep up with his sexual needs. \n",
      "\n",
      "I've been trying more to just give him a HJ or BJ if he is really frisky and I'm not, but sometimes I really am just emotionally drained from life and just want to be cuddled. I've explained this to him several times and he gets very frustrated and defensive and states he doesn't feel like I want to please him....which is not the case at all. I can see how sometimes he would feel this way because when he asks sometimes my \"tone\" may come across annoyed or uninterested. I don't want him to feel this way and it has been a problem for a while now. \n",
      "I want to learn how to approach his needs better without him feeling rejected, and I want us both to be happy.\n",
      "<|im_end|>\n",
      "\n",
      "Generated:\n",
      " system\n",
      "You are a helpful AI assistant that summarizes Reddit posts.\n",
      "user\n",
      "Summarize the following post:\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: Me [20/F] having trouble with boyfriend [23/m] of 2.5 years sex drive\n",
      "POST: My boyfriend and I have an amazing sex life. We are very sexually compatible.\n",
      "\n",
      "Although, his sex drive is crazy high compared to mine. It really depends on the time of month for me [hormones], sometimes I'll be down to go 3 times a day and sometimes I'm only interested in once a week. \n",
      "\n",
      "I'm asking for advice because I have a hard time denying him without him feeling rejected by me. For example, every time we cuddle he is almost always coming on to me. At night he comes on to me, in the middle of the night he masturbates next to me, and tries to come onto me in the morning. I'm utterly flattered that after being in a relationship this long he is still very turned on by me, but sometimes it is very overwhelming to keep up with his sexual needs. \n",
      "\n",
      "I've been trying more to just give him a HJ or BJ if he is really frisky and I'm not, but sometimes I really am just emotionally drained from life and just want to be cuddled. I've explained this to him several times and he gets very frustrated and defensive and states he doesn't feel like I want to please him....which is not the case at all. I can see how sometimes he would feel this way because when he asks sometimes my \"tone\" may come across annoyed or uninterested. I don't want him to feel this way and it has been a problem for a while now. \n",
      "I want to learn how to approach his needs better without him feeling rejected, and I want us both to be happy.\n",
      "\n",
      "Coherence: 4\n",
      "Consistency: 5\n",
      "Fluency: 3\n",
      "Relevance: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Pattern to catch Coherence/Consistency/Fluency used like scores\n",
    "score_pattern = re.compile(r\"^\\s*(Coherence|Consistency|Fluency)\\s*:\\s*\\d+(\\.\\d+)?\", re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "# Loop through dataset\n",
    "for i, example in enumerate(tqdm(dataset)):\n",
    "    # Remove TL;DR and format prompt\n",
    "    content = re.sub(r\"TL;DR:\\s*$\", \"\", example[\"prompt\"].strip())\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that summarizes Reddit posts.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the following post:\\n{content}\"}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "        decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Check for bad formatting\n",
    "    if score_pattern.search(decoded):\n",
    "        print(f\"\\n🚨 Problem found at index {i}\")\n",
    "        print(\"Prompt:\\n\", input_text)\n",
    "        print(\"Generated:\\n\", decoded)\n",
    "        break  # or remove this to continue scanning all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9deda73-a6d7-468d-8d96-fe10fac46588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Input:\n",
      " <|im_start|>system\n",
      "You are a helpful AI assistant that summarizes Reddit posts.<|im_end|>\n",
      "<|im_start|>user\n",
      "Summarize the following post:\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\n",
      "POST: Not sure if this belongs here but it's worth a try. \n",
      "\n",
      "Backstory:\n",
      "When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \n",
      "\n",
      "Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \n",
      "\n",
      "His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \n",
      "\n",
      "So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\n",
      "<|im_end|>\n",
      "\n",
      "DPO: system\n",
      "You are a helpful AI assistant that summarizes Reddit posts.\n",
      "user\n",
      "Summarize the following post:\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\n",
      "POST: Not sure if this belongs here but it's worth a try. \n",
      "\n",
      "Backstory:\n",
      "When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \n",
      "\n",
      "Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \n",
      "\n",
      "His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \n",
      "\n",
      "So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\n",
      "\n",
      "The original poster (OP), a woman (f/22), reflects on her past relationship, which led to a significant setback due to her ex-boyfriend needing space. She experienced severe emotional distress and decided to distance herself from him, severing all ties, including deleting his photos and removing his contact. Despite making progress over three years later, the memory of their time together still resurfaces, causing her to feel sadness and nostalgia. Her ex-girlfriends' presence on Facebook and occasional encounters at events makes her uncomfortable, fearing potential rejection or awkwardness. The OP considers cutting contact with these former friends to focus on her own healing journey, but wonders if she might offend them or experience discomfort. She seeks advice on whether this action aligns with ethical boundaries and whether she should expect any negative reactions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "# Your input content (without TL;DR)\n",
    "content = re.sub(r\"TL;DR:\\s*$\", \"\", dataset[0]['prompt'].strip())\n",
    "\n",
    "# Format it as a chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that summarizes Reddit posts.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following post:\\n{content}\"}\n",
    "]\n",
    "\n",
    "# 🔍 View the raw chat-formatted input\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(\"Formatted Input:\\n\", input_text)\n",
    "\n",
    "# ✅ Now tokenize it\n",
    "input_ids = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids.to(model.device)\n",
    "\n",
    "# 🔮 Generate output\n",
    "dpo_output = model.generate(input_ids, max_new_tokens=200)\n",
    "\n",
    "# 📝 Decode and print result\n",
    "print(\"DPO:\", tokenizer.decode(dpo_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2433394f-3cf2-42d2-9992-4e649bf28abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "tokenizer0 = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\")\n",
    "tokenizer0.pad_token = tokenizer0.eos_token  # make sure padding token is defined\n",
    "\n",
    "model0 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"thepowerfuldeez/Qwen2-1.5B-Summarize\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c8761f8-a3f9-4b18-8a8d-3e36366cd370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO: system\n",
      "You are a helpful AI assistant that summarizes Reddit posts.\n",
      "user\n",
      "Summarize the following post:\n",
      "SUBREDDIT: r/tifu\n",
      "TITLE: TIFU bY brushing with Baking Soda without learning how to do it correctly.\n",
      "POST: Always wanted White Teeth but never visited the dentist since I was 8 due to fear [gotten bad experience as a kid].        \n",
      "\n",
      "So I heard that baking soda makes your teeth white if you brush your teeth with it.        \n",
      "What I didn't get from all the reading, is that though it is supposed to be made into a paste, it shouldn't still be gritty.       \n",
      "\n",
      "I always kept my baking soda paste gritty by putting very little water.        \n",
      "\n",
      "After brushing straight with it for three months, my gum was extremely sore, but on the up side is, it is true, it is all true, I am amazed myself ! My teeth is very VERY white now compared to the past and even when taking pictures, the teeth becomes the center of attention simply because of how white it is, even my friends jokingly asked if I have painted it white.       \n",
      "These are the images after baking soda brushing for months, understand that I have NEVER visited a dentist ever since I was 8:    \n",
      "   \n",
      "\n",
      "As my ego grew, I forget about the irritation from the gum and keep on using it.      \n",
      "One fine day, my gum gave up...I was brushing and I saw a nice chunk of my gum get physically brushed OUT of my teeth, I was shocked and at a lost of what I should do...I tried to piece the gum back in hoping that it would stay, suffices to say by the very next day, the gum eventually fall off.       \n",
      "\n",
      "It is not that visible if I don't smile too big, but let this be a lesson to all of you out there, baking soda paste works, BUT PLEASE, make sure the paste is not gritty, PLEASE...don't experience this ever.\n",
      "\n",
      "The user shares a story about their attempt to whiten their teeth using baking soda, which they believed would result in super-white teeth. However, they discovered that the baking soda should be made into a paste, not gritty, and that after three months of brushing with it, their gums became sore and their teeth turned white. The user advises against using gritty baking soda paste and to avoid any dental issues.\n"
     ]
    }
   ],
   "source": [
    "# 🔮 Generate output\n",
    "dpo_output = model0.generate(input_ids, max_new_tokens=200)\n",
    "\n",
    "# 📝 Decode and print result\n",
    "print(\"DPO:\", tokenizer0.decode(dpo_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577a6246-caf4-48f0-a9e3-bca982fb6ca8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc5408062f142619ab8b2e59c27c17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/462 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd954f01e4b4aed9f05e26b9da852aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655be39dfb534234b8e2e6e770c13f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d490e8b034494db0f891e328654b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4f4da2100149ed897618cddd3f0bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e03c5b8887b4e12b5021fa7f89d56aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d36e30ee79141de8009831c8e0e0817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e58a3c05c5d436489106990c0ed4449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/92534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264ca89b400843a9b3e5ee2474d41113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/83629 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612452e48c4c4f97bfd7b12f2e376b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid1 split:   0%|          | 0/33082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46c924176314164a3188ffdae4cd4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid2 split:   0%|          | 0/50715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_cmp = load_dataset(\"CarperAI/openai_summarize_comparisons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61fccb23-a81d-415c-b2dd-ce4a9a4988ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"SUBREDDIT: r/relationships\\nTITLE: My [21/M] girlfriend [19/F] broke up with me after she went through my Facebook without my permission.\\nPOST: My girlfriend and I had been dating for 15 months. \\n\\n**Last week my girlfriend went onto my Facebook account and read through my message history with a couple of girls.**\\n\\nShe was **searching for a specific girl that I used to flirt with in the past, and she found it.**\\n\\nWe had fought one time before about me flirting with this girl, and I stopped talking to her entirely for a couple of months (obviously she didn't believe I did).\\n\\nShe found messages between the girl and I around my birthday in February, and her (message girl) birthday in June. Needless to say they were flirty but with no intentions of ever acting upon them. The girl lives in Europe and I live on the East Coast. But my girlfriend doesn't believe that I ever stopped talking to her, and that I was flirty throughout our entire relationship.\\n\\nI have no evidence to disprove this, except for the fact that I don't have her on social media anymore (excluding Facebook, which I now deleted)\\n\\nYes I know it was stupid for me to flirt in the first place, but I can't help but feel like there is a massive invasion of privacy and that she shouldn't have seen the messages in the first place.\",\n",
       " 'chosen': 'TL;DR:  My Girlfriend of 15 months went through my Facebook messages without my permission and found old conversations of me flirting with a girl. She broke up with me and went no contact.',\n",
       " 'rejected': 'TL;DR:  My girlfriend and I broke up after she went through my Facebook account without my permission.<|endoftext|>Citizens for the Republic'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cmp['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22cdfa-98c8-44d2-a554-20492521e05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da8f24-c142-44e8-88c1-683b7fceabbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79f2a277-2d93-437c-964f-4297305b8757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SUBREDDIT: r/relationships\\nTITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\\nPOST: Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\\nTL;DR: \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d92e16b-6598-4436-b410-cfa89b335bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO: SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\n",
      "POST: Not sure if this belongs here but it's worth a try. \n",
      "\n",
      "Backstory:\n",
      "When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \n",
      "\n",
      "Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \n",
      "\n",
      "His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \n",
      "\n",
      "So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\n",
      "TL;DR:  I have to figure out if I want to still know these girls or not and would hate to sound insulting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = dataset[0]['prompt']\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate output\n",
    "dpo_output = model.generate(input_ids, max_new_tokens=50)\n",
    "\n",
    "# Decode and print the result\n",
    "print(\"DPO:\", tokenizer.decode(dpo_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a31501-be15-4c6c-99b4-771a1bacfe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292e78c-761b-4a06-99c1-aca566314bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b7aaa2-5633-4298-bff2-97a0d57c0a24",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT: SUBREDDIT: r/relationships\n",
      "TITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\n",
      "POST: Hello hello everybody. I hope this isn't too trivial of a question to ask on here, but I've been feeling a bit out of my depth when it comes to this situation (I've had only one relationship before, and for many reasons, it was out of the ordinary).\n",
      "\n",
      "Okay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We're both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\n",
      "\n",
      "Okay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \n",
      "\n",
      "We ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\n",
      "\n",
      "Yesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I'm wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I've been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n",
      "TL;DR:  I slept with a guy on Facebook, and he said he wanted to \"play it by ear\" and not slap any labels on anything. I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /notebooks/trlx/examples/summarize_rlhf/ckpts/checkpoint_11000/hf_model were not used when initializing GPT2LMHeadModel: ['transformer.base_model.transformer.h.0.attn.c_attn.bias', 'transformer.base_model.transformer.h.0.attn.c_attn.weight', 'transformer.base_model.transformer.h.0.attn.c_proj.bias', 'transformer.base_model.transformer.h.0.attn.c_proj.weight', 'transformer.base_model.transformer.h.0.ln_1.bias', 'transformer.base_model.transformer.h.0.ln_1.weight', 'transformer.base_model.transformer.h.0.ln_2.bias', 'transformer.base_model.transformer.h.0.ln_2.weight', 'transformer.base_model.transformer.h.0.mlp.c_fc.bias', 'transformer.base_model.transformer.h.0.mlp.c_fc.weight', 'transformer.base_model.transformer.h.0.mlp.c_proj.bias', 'transformer.base_model.transformer.h.0.mlp.c_proj.weight', 'transformer.base_model.transformer.h.1.attn.c_attn.bias', 'transformer.base_model.transformer.h.1.attn.c_attn.weight', 'transformer.base_model.transformer.h.1.attn.c_proj.bias', 'transformer.base_model.transformer.h.1.attn.c_proj.weight', 'transformer.base_model.transformer.h.1.ln_1.bias', 'transformer.base_model.transformer.h.1.ln_1.weight', 'transformer.base_model.transformer.h.1.ln_2.bias', 'transformer.base_model.transformer.h.1.ln_2.weight', 'transformer.base_model.transformer.h.1.mlp.c_fc.bias', 'transformer.base_model.transformer.h.1.mlp.c_fc.weight', 'transformer.base_model.transformer.h.1.mlp.c_proj.bias', 'transformer.base_model.transformer.h.1.mlp.c_proj.weight', 'transformer.base_model.transformer.h.10.attn.c_attn.bias', 'transformer.base_model.transformer.h.10.attn.c_attn.weight', 'transformer.base_model.transformer.h.10.attn.c_proj.bias', 'transformer.base_model.transformer.h.10.attn.c_proj.weight', 'transformer.base_model.transformer.h.10.ln_1.bias', 'transformer.base_model.transformer.h.10.ln_1.weight', 'transformer.base_model.transformer.h.10.ln_2.bias', 'transformer.base_model.transformer.h.10.ln_2.weight', 'transformer.base_model.transformer.h.10.mlp.c_fc.bias', 'transformer.base_model.transformer.h.10.mlp.c_fc.weight', 'transformer.base_model.transformer.h.10.mlp.c_proj.bias', 'transformer.base_model.transformer.h.10.mlp.c_proj.weight', 'transformer.base_model.transformer.h.11.attn.c_attn.bias', 'transformer.base_model.transformer.h.11.attn.c_attn.weight', 'transformer.base_model.transformer.h.11.attn.c_proj.bias', 'transformer.base_model.transformer.h.11.attn.c_proj.weight', 'transformer.base_model.transformer.h.11.ln_1.bias', 'transformer.base_model.transformer.h.11.ln_1.weight', 'transformer.base_model.transformer.h.11.ln_2.bias', 'transformer.base_model.transformer.h.11.ln_2.weight', 'transformer.base_model.transformer.h.11.mlp.c_fc.bias', 'transformer.base_model.transformer.h.11.mlp.c_fc.weight', 'transformer.base_model.transformer.h.11.mlp.c_proj.bias', 'transformer.base_model.transformer.h.11.mlp.c_proj.weight', 'transformer.base_model.transformer.h.2.attn.c_attn.bias', 'transformer.base_model.transformer.h.2.attn.c_attn.weight', 'transformer.base_model.transformer.h.2.attn.c_proj.bias', 'transformer.base_model.transformer.h.2.attn.c_proj.weight', 'transformer.base_model.transformer.h.2.ln_1.bias', 'transformer.base_model.transformer.h.2.ln_1.weight', 'transformer.base_model.transformer.h.2.ln_2.bias', 'transformer.base_model.transformer.h.2.ln_2.weight', 'transformer.base_model.transformer.h.2.mlp.c_fc.bias', 'transformer.base_model.transformer.h.2.mlp.c_fc.weight', 'transformer.base_model.transformer.h.2.mlp.c_proj.bias', 'transformer.base_model.transformer.h.2.mlp.c_proj.weight', 'transformer.base_model.transformer.h.3.attn.c_attn.bias', 'transformer.base_model.transformer.h.3.attn.c_attn.weight', 'transformer.base_model.transformer.h.3.attn.c_proj.bias', 'transformer.base_model.transformer.h.3.attn.c_proj.weight', 'transformer.base_model.transformer.h.3.ln_1.bias', 'transformer.base_model.transformer.h.3.ln_1.weight', 'transformer.base_model.transformer.h.3.ln_2.bias', 'transformer.base_model.transformer.h.3.ln_2.weight', 'transformer.base_model.transformer.h.3.mlp.c_fc.bias', 'transformer.base_model.transformer.h.3.mlp.c_fc.weight', 'transformer.base_model.transformer.h.3.mlp.c_proj.bias', 'transformer.base_model.transformer.h.3.mlp.c_proj.weight', 'transformer.base_model.transformer.h.4.attn.c_attn.bias', 'transformer.base_model.transformer.h.4.attn.c_attn.weight', 'transformer.base_model.transformer.h.4.attn.c_proj.bias', 'transformer.base_model.transformer.h.4.attn.c_proj.weight', 'transformer.base_model.transformer.h.4.ln_1.bias', 'transformer.base_model.transformer.h.4.ln_1.weight', 'transformer.base_model.transformer.h.4.ln_2.bias', 'transformer.base_model.transformer.h.4.ln_2.weight', 'transformer.base_model.transformer.h.4.mlp.c_fc.bias', 'transformer.base_model.transformer.h.4.mlp.c_fc.weight', 'transformer.base_model.transformer.h.4.mlp.c_proj.bias', 'transformer.base_model.transformer.h.4.mlp.c_proj.weight', 'transformer.base_model.transformer.h.5.attn.c_attn.bias', 'transformer.base_model.transformer.h.5.attn.c_attn.weight', 'transformer.base_model.transformer.h.5.attn.c_proj.bias', 'transformer.base_model.transformer.h.5.attn.c_proj.weight', 'transformer.base_model.transformer.h.5.ln_1.bias', 'transformer.base_model.transformer.h.5.ln_1.weight', 'transformer.base_model.transformer.h.5.ln_2.bias', 'transformer.base_model.transformer.h.5.ln_2.weight', 'transformer.base_model.transformer.h.5.mlp.c_fc.bias', 'transformer.base_model.transformer.h.5.mlp.c_fc.weight', 'transformer.base_model.transformer.h.5.mlp.c_proj.bias', 'transformer.base_model.transformer.h.5.mlp.c_proj.weight', 'transformer.base_model.transformer.h.6.attn.c_attn.bias', 'transformer.base_model.transformer.h.6.attn.c_attn.weight', 'transformer.base_model.transformer.h.6.attn.c_proj.bias', 'transformer.base_model.transformer.h.6.attn.c_proj.weight', 'transformer.base_model.transformer.h.6.ln_1.bias', 'transformer.base_model.transformer.h.6.ln_1.weight', 'transformer.base_model.transformer.h.6.ln_2.bias', 'transformer.base_model.transformer.h.6.ln_2.weight', 'transformer.base_model.transformer.h.6.mlp.c_fc.bias', 'transformer.base_model.transformer.h.6.mlp.c_fc.weight', 'transformer.base_model.transformer.h.6.mlp.c_proj.bias', 'transformer.base_model.transformer.h.6.mlp.c_proj.weight', 'transformer.base_model.transformer.h.7.attn.c_attn.bias', 'transformer.base_model.transformer.h.7.attn.c_attn.weight', 'transformer.base_model.transformer.h.7.attn.c_proj.bias', 'transformer.base_model.transformer.h.7.attn.c_proj.weight', 'transformer.base_model.transformer.h.7.ln_1.bias', 'transformer.base_model.transformer.h.7.ln_1.weight', 'transformer.base_model.transformer.h.7.ln_2.bias', 'transformer.base_model.transformer.h.7.ln_2.weight', 'transformer.base_model.transformer.h.7.mlp.c_fc.bias', 'transformer.base_model.transformer.h.7.mlp.c_fc.weight', 'transformer.base_model.transformer.h.7.mlp.c_proj.bias', 'transformer.base_model.transformer.h.7.mlp.c_proj.weight', 'transformer.base_model.transformer.h.8.attn.c_attn.bias', 'transformer.base_model.transformer.h.8.attn.c_attn.weight', 'transformer.base_model.transformer.h.8.attn.c_proj.bias', 'transformer.base_model.transformer.h.8.attn.c_proj.weight', 'transformer.base_model.transformer.h.8.ln_1.bias', 'transformer.base_model.transformer.h.8.ln_1.weight', 'transformer.base_model.transformer.h.8.ln_2.bias', 'transformer.base_model.transformer.h.8.ln_2.weight', 'transformer.base_model.transformer.h.8.mlp.c_fc.bias', 'transformer.base_model.transformer.h.8.mlp.c_fc.weight', 'transformer.base_model.transformer.h.8.mlp.c_proj.bias', 'transformer.base_model.transformer.h.8.mlp.c_proj.weight', 'transformer.base_model.transformer.h.9.attn.c_attn.bias', 'transformer.base_model.transformer.h.9.attn.c_attn.weight', 'transformer.base_model.transformer.h.9.attn.c_proj.bias', 'transformer.base_model.transformer.h.9.attn.c_proj.weight', 'transformer.base_model.transformer.h.9.ln_1.bias', 'transformer.base_model.transformer.h.9.ln_1.weight', 'transformer.base_model.transformer.h.9.ln_2.bias', 'transformer.base_model.transformer.h.9.ln_2.weight', 'transformer.base_model.transformer.h.9.mlp.c_fc.bias', 'transformer.base_model.transformer.h.9.mlp.c_fc.weight', 'transformer.base_model.transformer.h.9.mlp.c_proj.bias', 'transformer.base_model.transformer.h.9.mlp.c_proj.weight', 'transformer.base_model.transformer.ln_f.bias', 'transformer.base_model.transformer.ln_f.weight', 'transformer.base_model.transformer.wpe.weight', 'transformer.base_model.transformer.wte.weight', 'transformer.frozen_head.decoder_blocks.0.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.0.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.0.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.0.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.0.ln_1.bias', 'transformer.frozen_head.decoder_blocks.0.ln_1.weight', 'transformer.frozen_head.decoder_blocks.0.ln_2.bias', 'transformer.frozen_head.decoder_blocks.0.ln_2.weight', 'transformer.frozen_head.decoder_blocks.0.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.0.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.0.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.0.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.1.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.1.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.1.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.1.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.1.ln_1.bias', 'transformer.frozen_head.decoder_blocks.1.ln_1.weight', 'transformer.frozen_head.decoder_blocks.1.ln_2.bias', 'transformer.frozen_head.decoder_blocks.1.ln_2.weight', 'transformer.frozen_head.decoder_blocks.1.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.1.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.1.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.1.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.2.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.2.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.2.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.2.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.2.ln_1.bias', 'transformer.frozen_head.decoder_blocks.2.ln_1.weight', 'transformer.frozen_head.decoder_blocks.2.ln_2.bias', 'transformer.frozen_head.decoder_blocks.2.ln_2.weight', 'transformer.frozen_head.decoder_blocks.2.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.2.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.2.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.2.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.3.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.3.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.3.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.3.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.3.ln_1.bias', 'transformer.frozen_head.decoder_blocks.3.ln_1.weight', 'transformer.frozen_head.decoder_blocks.3.ln_2.bias', 'transformer.frozen_head.decoder_blocks.3.ln_2.weight', 'transformer.frozen_head.decoder_blocks.3.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.3.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.3.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.3.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.4.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.4.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.4.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.4.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.4.ln_1.bias', 'transformer.frozen_head.decoder_blocks.4.ln_1.weight', 'transformer.frozen_head.decoder_blocks.4.ln_2.bias', 'transformer.frozen_head.decoder_blocks.4.ln_2.weight', 'transformer.frozen_head.decoder_blocks.4.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.4.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.4.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.4.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.5.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.5.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.5.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.5.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.5.ln_1.bias', 'transformer.frozen_head.decoder_blocks.5.ln_1.weight', 'transformer.frozen_head.decoder_blocks.5.ln_2.bias', 'transformer.frozen_head.decoder_blocks.5.ln_2.weight', 'transformer.frozen_head.decoder_blocks.5.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.5.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.5.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.5.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.6.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.6.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.6.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.6.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.6.ln_1.bias', 'transformer.frozen_head.decoder_blocks.6.ln_1.weight', 'transformer.frozen_head.decoder_blocks.6.ln_2.bias', 'transformer.frozen_head.decoder_blocks.6.ln_2.weight', 'transformer.frozen_head.decoder_blocks.6.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.6.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.6.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.6.mlp.c_proj.weight', 'transformer.frozen_head.decoder_blocks.7.attn.c_attn.bias', 'transformer.frozen_head.decoder_blocks.7.attn.c_attn.weight', 'transformer.frozen_head.decoder_blocks.7.attn.c_proj.bias', 'transformer.frozen_head.decoder_blocks.7.attn.c_proj.weight', 'transformer.frozen_head.decoder_blocks.7.ln_1.bias', 'transformer.frozen_head.decoder_blocks.7.ln_1.weight', 'transformer.frozen_head.decoder_blocks.7.ln_2.bias', 'transformer.frozen_head.decoder_blocks.7.ln_2.weight', 'transformer.frozen_head.decoder_blocks.7.mlp.c_fc.bias', 'transformer.frozen_head.decoder_blocks.7.mlp.c_fc.weight', 'transformer.frozen_head.decoder_blocks.7.mlp.c_proj.bias', 'transformer.frozen_head.decoder_blocks.7.mlp.c_proj.weight', 'transformer.frozen_head.final_norm.bias', 'transformer.frozen_head.final_norm.weight', 'transformer.frozen_head.lm_head.weight', 'transformer.v_head.0.bias', 'transformer.v_head.0.weight', 'transformer.v_head.2.bias', 'transformer.v_head.2.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /notebooks/trlx/examples/summarize_rlhf/ckpts/checkpoint_11000/hf_model and are newly initialized: ['lm_head.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wpe.weight', 'transformer.wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO: SUBREDDIT: r/relationships\n",
      "TITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\n",
      "POST: Hello hello everybody. I hope this isn't too trivial of a question to ask on here, but I've been feeling a bit out of my depth when it comes to this situation (I've had only one relationship before, and for many reasons, it was out of the ordinary).\n",
      "\n",
      "Okay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We're both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\n",
      "\n",
      "Okay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \n",
      "\n",
      "We ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\n",
      "\n",
      "Yesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I'm wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I've been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n",
      "TL;DR:  integ organsGTGTGTGTGTGTGTGTGT Univ Univ Gingrich Gingrich Gingrich Gingrich Gingrich Gingrich Gingrich Gingrich Gingrich Notting Notting NottingBRGTGT Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs Customs\n"
     ]
    }
   ],
   "source": [
    "input_text = dataset[0]['prompt']\n",
    "# SFT model\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\"/notebooks/gpt2-supervised-summarize-checkpoint/checkpoint-29000\").to(device)\n",
    "sft_output = sft_model.generate(tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=50)\n",
    "print(\"SFT:\", tokenizer.decode(sft_output[0], skip_special_tokens=True))\n",
    "\n",
    "# PPO model\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained(\"/notebooks/trlx/examples/summarize_rlhf/ckpts/checkpoint_11000/hf_model\").to(device)\n",
    "ppo_output = ppo_model.generate(tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=50)\n",
    "print(\"PPO:\", tokenizer.decode(ppo_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993e833-cc0a-4c4e-a238-8bcd4caad6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33940cd0-049c-4967-b2a9-50617dfaa4fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at .Trash-0/files/checkpoint_01000/hf_model were not used when initializing GPT2LMHeadModel: ['base_model.transformer.h.2.ln_1.weight', 'frozen_head.decoder_blocks.1.mlp.c_fc.bias', 'frozen_head.decoder_blocks.2.ln_1.weight', 'base_model.transformer.h.10.mlp.c_fc.bias', 'base_model.transformer.h.0.mlp.c_fc.weight', 'base_model.transformer.h.3.mlp.c_fc.bias', 'base_model.transformer.h.7.ln_1.weight', 'frozen_head.decoder_blocks.3.mlp.c_fc.weight', 'base_model.transformer.h.8.mlp.c_proj.weight', 'v_head.2.weight', 'base_model.transformer.h.4.mlp.c_fc.bias', 'base_model.transformer.h.2.ln_2.weight', 'frozen_head.decoder_blocks.2.attn.c_proj.weight', 'base_model.transformer.h.0.ln_2.bias', 'frozen_head.decoder_blocks.1.attn.c_proj.bias', 'base_model.transformer.h.3.ln_1.bias', 'base_model.transformer.h.3.mlp.c_fc.weight', 'base_model.transformer.h.8.mlp.c_fc.bias', 'base_model.transformer.h.3.mlp.c_proj.weight', 'base_model.transformer.h.0.attn.c_proj.bias', 'base_model.transformer.h.4.mlp.c_proj.weight', 'base_model.transformer.h.0.attn.c_proj.weight', 'base_model.transformer.h.9.mlp.c_fc.weight', 'base_model.transformer.h.11.ln_2.weight', 'base_model.transformer.h.2.mlp.c_fc.bias', 'base_model.transformer.h.1.ln_1.bias', 'base_model.transformer.h.1.attn.c_proj.bias', 'frozen_head.decoder_blocks.0.attn.c_attn.weight', 'v_head.2.bias', 'frozen_head.decoder_blocks.1.mlp.c_proj.weight', 'base_model.transformer.h.3.attn.c_attn.bias', 'base_model.transformer.h.3.attn.c_attn.weight', 'base_model.transformer.h.9.attn.c_proj.bias', 'base_model.transformer.h.8.attn.c_proj.weight', 'base_model.transformer.h.8.mlp.c_proj.bias', 'base_model.transformer.h.9.mlp.c_fc.bias', 'base_model.transformer.h.8.ln_1.weight', 'base_model.transformer.h.6.mlp.c_fc.bias', 'v_head.0.bias', 'frozen_head.decoder_blocks.1.mlp.c_fc.weight', 'base_model.transformer.h.4.ln_1.weight', 'frozen_head.decoder_blocks.3.attn.c_attn.bias', 'base_model.transformer.h.7.ln_1.bias', 'base_model.transformer.h.1.attn.c_attn.bias', 'frozen_head.decoder_blocks.2.mlp.c_fc.weight', 'frozen_head.decoder_blocks.2.ln_1.bias', 'frozen_head.decoder_blocks.0.mlp.c_fc.weight', 'base_model.transformer.h.2.ln_2.bias', 'base_model.transformer.h.8.attn.c_proj.bias', 'base_model.transformer.h.8.attn.c_attn.weight', 'base_model.transformer.wpe.weight', 'frozen_head.decoder_blocks.0.attn.c_proj.bias', 'base_model.transformer.h.5.ln_2.bias', 'frozen_head.lm_head.weight', 'v_head.0.weight', 'base_model.transformer.h.2.mlp.c_proj.weight', 'base_model.transformer.h.1.mlp.c_fc.bias', 'frozen_head.decoder_blocks.2.ln_2.bias', 'base_model.transformer.h.3.attn.c_proj.bias', 'base_model.transformer.h.7.attn.c_proj.bias', 'base_model.transformer.h.6.mlp.c_fc.weight', 'base_model.transformer.h.10.attn.c_proj.weight', 'base_model.transformer.h.5.mlp.c_fc.bias', 'base_model.transformer.h.11.mlp.c_fc.weight', 'base_model.transformer.h.9.ln_1.weight', 'frozen_head.decoder_blocks.0.ln_1.bias', 'frozen_head.decoder_blocks.1.ln_2.weight', 'frozen_head.decoder_blocks.3.ln_1.bias', 'base_model.transformer.h.11.mlp.c_proj.bias', 'base_model.transformer.h.9.mlp.c_proj.weight', 'frozen_head.decoder_blocks.2.attn.c_attn.weight', 'base_model.transformer.h.2.mlp.c_proj.bias', 'base_model.transformer.h.6.ln_2.weight', 'base_model.transformer.h.7.attn.c_proj.weight', 'frozen_head.decoder_blocks.0.ln_2.bias', 'frozen_head.decoder_blocks.2.ln_2.weight', 'base_model.transformer.h.9.attn.c_attn.bias', 'base_model.transformer.h.11.attn.c_proj.weight', 'base_model.transformer.h.9.ln_2.bias', 'base_model.transformer.ln_f.weight', 'base_model.transformer.h.8.mlp.c_fc.weight', 'base_model.transformer.h.10.attn.c_proj.bias', 'frozen_head.decoder_blocks.1.attn.c_attn.bias', 'base_model.transformer.h.9.attn.c_proj.weight', 'base_model.transformer.h.1.mlp.c_fc.weight', 'frozen_head.decoder_blocks.1.ln_1.bias', 'base_model.transformer.h.3.ln_1.weight', 'base_model.transformer.h.3.ln_2.bias', 'base_model.transformer.h.9.mlp.c_proj.bias', 'base_model.transformer.h.8.attn.c_attn.bias', 'base_model.transformer.h.6.attn.c_proj.weight', 'frozen_head.decoder_blocks.1.ln_2.bias', 'base_model.transformer.h.7.ln_2.weight', 'base_model.transformer.h.6.ln_2.bias', 'base_model.transformer.h.8.ln_2.weight', 'base_model.transformer.h.0.mlp.c_fc.bias', 'base_model.transformer.h.5.attn.c_proj.weight', 'base_model.transformer.h.4.attn.c_attn.weight', 'base_model.transformer.h.1.ln_2.bias', 'frozen_head.decoder_blocks.1.attn.c_proj.weight', 'frozen_head.decoder_blocks.2.attn.c_attn.bias', 'frozen_head.decoder_blocks.0.attn.c_attn.bias', 'base_model.transformer.h.8.ln_2.bias', 'base_model.transformer.h.4.ln_2.bias', 'base_model.transformer.h.7.mlp.c_proj.weight', 'base_model.transformer.h.9.attn.c_attn.weight', 'base_model.transformer.h.5.mlp.c_proj.bias', 'base_model.transformer.h.6.attn.c_attn.bias', 'base_model.transformer.h.2.ln_1.bias', 'base_model.transformer.h.2.attn.c_attn.weight', 'base_model.transformer.h.4.mlp.c_proj.bias', 'base_model.transformer.h.5.attn.c_attn.bias', 'base_model.transformer.h.0.attn.c_attn.weight', 'base_model.transformer.h.0.attn.c_attn.bias', 'base_model.transformer.h.4.ln_2.weight', 'base_model.transformer.h.11.ln_2.bias', 'base_model.transformer.h.5.ln_1.bias', 'base_model.transformer.h.1.mlp.c_proj.weight', 'base_model.transformer.h.1.ln_1.weight', 'frozen_head.decoder_blocks.2.mlp.c_fc.bias', 'base_model.transformer.h.6.mlp.c_proj.weight', 'frozen_head.decoder_blocks.1.mlp.c_proj.bias', 'base_model.transformer.h.0.mlp.c_proj.weight', 'base_model.transformer.h.10.attn.c_attn.weight', 'frozen_head.decoder_blocks.3.mlp.c_fc.bias', 'base_model.transformer.h.2.attn.c_attn.bias', 'frozen_head.decoder_blocks.3.attn.c_proj.weight', 'base_model.transformer.h.0.ln_1.weight', 'base_model.transformer.h.11.mlp.c_proj.weight', 'base_model.transformer.h.6.ln_1.weight', 'base_model.transformer.h.6.mlp.c_proj.bias', 'base_model.transformer.h.10.mlp.c_fc.weight', 'base_model.transformer.h.5.attn.c_proj.bias', 'base_model.transformer.h.5.mlp.c_fc.weight', 'base_model.transformer.h.4.attn.c_proj.weight', 'base_model.transformer.h.10.mlp.c_proj.weight', 'base_model.transformer.h.5.mlp.c_proj.weight', 'base_model.transformer.h.6.attn.c_attn.weight', 'base_model.transformer.h.10.ln_2.bias', 'base_model.transformer.h.5.ln_1.weight', 'frozen_head.decoder_blocks.3.ln_1.weight', 'frozen_head.decoder_blocks.0.ln_1.weight', 'base_model.transformer.h.1.ln_2.weight', 'base_model.transformer.h.7.ln_2.bias', 'frozen_head.decoder_blocks.1.attn.c_attn.weight', 'frozen_head.decoder_blocks.2.mlp.c_proj.weight', 'frozen_head.decoder_blocks.0.attn.c_proj.weight', 'base_model.transformer.h.2.attn.c_proj.weight', 'base_model.transformer.h.11.attn.c_proj.bias', 'frozen_head.decoder_blocks.3.mlp.c_proj.bias', 'frozen_head.decoder_blocks.3.ln_2.bias', 'base_model.transformer.h.11.mlp.c_fc.bias', 'base_model.transformer.h.11.attn.c_attn.weight', 'base_model.transformer.h.0.ln_2.weight', 'base_model.transformer.h.10.mlp.c_proj.bias', 'base_model.transformer.wte.weight', 'frozen_head.decoder_blocks.1.ln_1.weight', 'base_model.transformer.h.1.attn.c_attn.weight', 'frozen_head.decoder_blocks.0.ln_2.weight', 'base_model.transformer.h.10.attn.c_attn.bias', 'frozen_head.decoder_blocks.2.mlp.c_proj.bias', 'base_model.transformer.h.10.ln_1.weight', 'frozen_head.decoder_blocks.3.attn.c_proj.bias', 'base_model.transformer.h.11.ln_1.weight', 'base_model.transformer.h.6.attn.c_proj.bias', 'base_model.transformer.h.3.ln_2.weight', 'base_model.transformer.h.8.ln_1.bias', 'base_model.transformer.h.1.attn.c_proj.weight', 'base_model.transformer.h.7.attn.c_attn.weight', 'base_model.transformer.h.4.attn.c_attn.bias', 'frozen_head.decoder_blocks.3.mlp.c_proj.weight', 'base_model.transformer.h.4.attn.c_proj.bias', 'base_model.transformer.h.4.ln_1.bias', 'base_model.transformer.h.3.attn.c_proj.weight', 'base_model.transformer.h.1.mlp.c_proj.bias', 'base_model.transformer.h.0.ln_1.bias', 'base_model.transformer.h.9.ln_2.weight', 'base_model.transformer.h.3.mlp.c_proj.bias', 'base_model.transformer.ln_f.bias', 'base_model.transformer.h.0.mlp.c_proj.bias', 'frozen_head.decoder_blocks.2.attn.c_proj.bias', 'base_model.transformer.h.7.attn.c_attn.bias', 'base_model.transformer.h.2.attn.c_proj.bias', 'base_model.transformer.h.11.attn.c_attn.bias', 'base_model.transformer.h.5.ln_2.weight', 'base_model.transformer.h.7.mlp.c_fc.bias', 'base_model.transformer.h.5.attn.c_attn.weight', 'frozen_head.decoder_blocks.3.ln_2.weight', 'frozen_head.decoder_blocks.0.mlp.c_fc.bias', 'base_model.transformer.h.7.mlp.c_fc.weight', 'base_model.transformer.h.7.mlp.c_proj.bias', 'base_model.transformer.h.6.ln_1.bias', 'base_model.transformer.h.9.ln_1.bias', 'base_model.transformer.h.4.mlp.c_fc.weight', 'frozen_head.decoder_blocks.0.mlp.c_proj.weight', 'base_model.transformer.h.10.ln_1.bias', 'base_model.transformer.h.2.mlp.c_fc.weight', 'base_model.transformer.h.11.ln_1.bias', 'frozen_head.decoder_blocks.3.attn.c_attn.weight', 'frozen_head.final_norm.bias', 'base_model.transformer.h.10.ln_2.weight', 'frozen_head.final_norm.weight', 'frozen_head.decoder_blocks.0.mlp.c_proj.bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at .Trash-0/files/checkpoint_01000/hf_model and are newly initialized: ['h.0.ln_1.bias', 'h.1.mlp.c_proj.bias', 'h.7.ln_1.bias', 'h.7.mlp.c_fc.bias', 'h.4.attn.c_proj.weight', 'h.6.mlp.c_fc.bias', 'h.11.attn.c_proj.bias', 'h.1.ln_1.bias', 'h.4.ln_1.bias', 'h.0.attn.c_proj.weight', 'h.6.attn.c_attn.bias', 'h.8.attn.c_attn.weight', 'h.3.mlp.c_fc.weight', 'h.1.ln_2.bias', 'h.9.ln_1.bias', 'h.10.attn.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.11.mlp.c_proj.bias', 'h.11.ln_1.bias', 'h.9.ln_2.bias', 'h.6.attn.c_proj.bias', 'h.4.ln_2.bias', 'h.3.ln_2.bias', 'h.6.ln_2.weight', 'h.3.attn.c_attn.bias', 'h.7.ln_2.bias', 'h.6.ln_1.bias', 'h.6.attn.c_proj.weight', 'h.4.ln_1.weight', 'h.0.mlp.c_fc.weight', 'h.5.ln_1.bias', 'h.9.mlp.c_proj.bias', 'h.3.attn.c_proj.bias', 'h.5.attn.c_attn.bias', 'h.9.ln_1.weight', 'h.5.attn.c_proj.bias', 'h.10.mlp.c_fc.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.5.mlp.c_fc.bias', 'h.7.ln_1.weight', 'h.0.attn.c_attn.weight', 'h.5.attn.c_proj.weight', 'h.11.mlp.c_fc.bias', 'h.2.ln_1.weight', 'h.10.attn.c_attn.bias', 'h.3.mlp.c_proj.bias', 'ln_f.weight', 'h.7.attn.c_proj.weight', 'h.1.ln_2.weight', 'h.5.attn.c_attn.weight', 'h.9.mlp.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.6.mlp.c_fc.weight', 'h.8.ln_1.weight', 'h.0.ln_2.bias', 'h.10.ln_1.bias', 'h.11.ln_1.weight', 'h.11.mlp.c_proj.weight', 'h.7.attn.c_attn.bias', 'h.7.ln_2.weight', 'h.11.attn.c_attn.weight', 'h.1.mlp.c_fc.weight', 'h.1.attn.c_attn.bias', 'h.2.attn.c_attn.weight', 'h.1.mlp.c_proj.weight', 'h.11.ln_2.bias', 'h.0.attn.c_proj.bias', 'h.8.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.8.attn.c_proj.bias', 'h.6.mlp.c_proj.bias', 'h.3.mlp.c_fc.bias', 'h.1.attn.c_proj.weight', 'h.5.mlp.c_fc.weight', 'h.11.attn.c_attn.bias', 'h.8.ln_2.weight', 'h.3.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.10.ln_2.weight', 'h.3.attn.c_attn.weight', 'h.9.ln_2.weight', 'h.6.ln_1.weight', 'h.0.mlp.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.6.mlp.c_proj.weight', 'h.1.ln_1.weight', 'wte.weight', 'h.3.ln_2.weight', 'h.1.attn.c_proj.bias', 'h.5.mlp.c_proj.weight', 'h.11.ln_2.weight', 'h.8.mlp.c_fc.weight', 'h.0.ln_1.weight', 'h.8.ln_2.bias', 'h.0.ln_2.weight', 'h.3.ln_1.weight', 'h.10.attn.c_proj.weight', 'h.4.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.2.ln_1.bias', 'h.4.attn.c_proj.bias', 'h.8.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.2.ln_2.bias', 'h.2.attn.c_attn.bias', 'h.5.ln_2.weight', 'h.7.mlp.c_proj.bias', 'h.5.mlp.c_proj.bias', 'ln_f.bias', 'h.7.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.6.ln_2.bias', 'h.8.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'lm_head.weight', 'h.7.mlp.c_proj.weight', 'h.10.ln_2.bias', 'h.9.attn.c_attn.weight', 'h.4.mlp.c_fc.weight', 'h.10.attn.c_attn.weight', 'h.10.mlp.c_proj.bias', 'h.8.attn.c_attn.bias', 'h.2.attn.c_proj.bias', 'h.9.attn.c_attn.bias', 'h.5.ln_2.bias', 'h.2.mlp.c_fc.weight', 'wpe.weight', 'h.3.mlp.c_proj.weight', 'h.4.mlp.c_proj.weight', 'h.2.ln_2.weight', 'h.3.attn.c_proj.weight', 'h.5.ln_1.weight', 'h.8.ln_1.bias', 'h.9.mlp.c_fc.weight', 'h.10.mlp.c_fc.weight', 'h.0.attn.c_attn.bias', 'h.7.attn.c_attn.weight', 'h.8.mlp.c_fc.bias', 'h.2.mlp.c_fc.bias', 'h.10.ln_1.weight', 'h.7.mlp.c_fc.weight', 'h.6.attn.c_attn.weight', 'h.0.mlp.c_fc.bias', 'h.1.mlp.c_fc.bias', 'h.1.attn.c_attn.weight', 'h.11.attn.c_proj.weight', 'h.10.mlp.c_proj.weight', 'h.9.attn.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".Trash-0/files/checkpoint_01000/hf_model\",\n",
    "    # ignore_mismatched_sizes=True  # prevents warnings from crashing things\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f683fe8-bf81-4abd-a408-b2f228a28003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  #\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642be362-5686-446a-97c3-06c04a8b8d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_dtype=torch.float16,\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc628c30-8cfb-4cc8-bc88-e0fb1a455c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
