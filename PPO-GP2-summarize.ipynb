{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debb4d3b-266e-445f-afd6-c73548d5749f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: datasets 2.11.0\n",
      "Uninstalling datasets-2.11.0:\n",
      "  Successfully uninstalled datasets-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall datasets -y\n",
    "%pip install datasets>=3.0.0\n",
    "!pip install torchdata==0.6.0\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    peft==0.3.0 --quiet\n",
    "\n",
    "# Installing the Reinforcement Learning library directly from github.\n",
    "%pip install git+https://github.com/lvwerra/trl.git@25fa1bd \n",
    "%pip install huggingface_hub[hf_xet]\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22da35b5-997a-4a28-8d27-c915c94d2c70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0764fad-2b3f-4598-96a9-e7c8d66181fd",
   "metadata": {},
   "source": [
    "### Load a Pretrained Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49caeb1e-2d34-468a-b8e6-5c01d9b17fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SFT_MODEL_PATH = \"KookyGhost/GPT2-small-summarization\"\n",
    "model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH).to(\"cuda\")  # or \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575eed16-36a6-4c53-a840-3c8d63004bf6",
   "metadata": {},
   "source": [
    "### Turn Dataset into the Appropriate Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9baa85e-a5a8-46ee-ae99-a9d0207875cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length, \n",
    "                  input_max_text_length):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and return train/valid/test splits with input_ids.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Name or path of the tokenizer/model.\n",
    "    - dataset_name (str): Name of the Hugging Face dataset.\n",
    "    - input_min_text_length (int): Minimum character length of prompt.\n",
    "    - input_max_text_length (int): Maximum character length of prompt.\n",
    "\n",
    "    Returns:\n",
    "    - dataset (datasets.DatasetDict): Tokenized dataset with train/valid/test splits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load all splits\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess(split):\n",
    "        # Filter by character length of prompt\n",
    "        split = split.filter(\n",
    "            lambda x: input_min_text_length < len(x[\"prompt\"]) <= input_max_text_length,\n",
    "            batched=False\n",
    "        )\n",
    "\n",
    "        def tokenize(sample):\n",
    "            prompt = f\"{sample['prompt']}\\n\\n\"\n",
    "            inputs = tokenizer(prompt, truncation=True, max_length=1024)\n",
    "\n",
    "            sample[\"input_ids\"] = inputs[\"input_ids\"]\n",
    "            sample[\"query\"] = tokenizer.decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
    "            return sample\n",
    "\n",
    "        split = split.map(tokenize, batched=False)\n",
    "        split.set_format(type=\"torch\")\n",
    "        return split\n",
    "\n",
    "    dataset[\"train\"] = preprocess(dataset[\"train\"])\n",
    "    dataset[\"valid\"] = preprocess(dataset[\"valid\"])\n",
    "    dataset[\"test\"]  = preprocess(dataset[\"test\"])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170305c5-680b-4681-a7c9-293a6d063da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1246/1246 [00:00<00:00, 1250.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(\n",
    "    model_name=\"gpt2\",\n",
    "    dataset_name=\"CarperAI/openai_summarize_tldr\",\n",
    "    input_min_text_length=200,\n",
    "    input_max_text_length=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53942d7-5b68-404f-9616-5c32e660a753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'label', 'input_ids', 'query'],\n",
       "    num_rows: 22252\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset(\"CarperAI/openai_summarize_tldr\") \n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5f93199-e6d0-4f26-8949-a28d4ca7bcf5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9f81d-03a7-4689-a001-301a86362cba",
   "metadata": {},
   "source": [
    "### Check Trainable Parameters and Make Sure Reference Model is Not Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d39ef23-4f2e-46ea-8c17-214a03deb5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5446e226-0b98-429d-b038-299184ecc133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! git clone https://huggingface.co/z7ye/peft-dialogue-summary-checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1626984b-7bd8-49e0-b10b-6bd9e1d9696a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 124440577\n",
      "all model parameters: 124440577\n",
      "percentage of trainable model parameters: 100.00%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(SFT_MODEL_PATH,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b65ff83b-f63c-414d-bf0d-b44fe5e1ce56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 124440577\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d910f-8a28-4c31-8bde-c3c3ae1ba33b",
   "metadata": {},
   "source": [
    "### Load Reward Model from OpenAssistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf2ff1f-8fc2-45cf-9a6b-8bae5d431b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0'}\n"
     ]
    }
   ],
   "source": [
    "rw_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "rw_tokenizer = AutoTokenizer.from_pretrained(rw_model_name)\n",
    "rw_model = AutoModelForSequenceClassification.from_pretrained(rw_model_name)\n",
    "rw_model.to(\"cuda\")\n",
    "print(rw_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44cb02-c884-4ffd-a90e-6c0df627d968",
   "metadata": {},
   "source": [
    "### Define Reward Evaluation Function for PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d792935e-4de5-40fe-8439-7b5e1507646a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_normalized_reward_score(model, \n",
    "                                     reward_model, \n",
    "                                     reward_tokenizer, \n",
    "                                     dataset, \n",
    "                                     tokenizer, \n",
    "                                     num_samples):\n",
    "    \"\"\"\n",
    "    Generate responses from the model and normalize their reward by reference label score.\n",
    "    \"\"\"\n",
    "    max_new_tokens = 100\n",
    "    rewards = []\n",
    "\n",
    "    reward_model.eval()\n",
    "\n",
    "    for i, sample in tqdm(enumerate(dataset), total=min(num_samples, len(dataset))):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        input_text = sample[\"query\"]\n",
    "        reference_summary = sample[\"label\"]\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            top_p=1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Generate model output\n",
    "        response_token_ids = model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Prepare full inputs\n",
    "        full_gen_input = f\"{input_text}\\n{generated_text}\"\n",
    "        full_ref_input = f\"{input_text}\\n{reference_summary}\"\n",
    "\n",
    "        # Tokenize both\n",
    "        gen_inputs = rw_tokenizer(full_gen_input, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "        ref_inputs = rw_tokenizer(full_ref_input, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_logits = reward_model(**gen_inputs).logits\n",
    "            ref_logits = reward_model(**ref_inputs).logits\n",
    "\n",
    "            gen_reward = sigmoid(gen_logits).item()\n",
    "            ref_reward = sigmoid(ref_logits).item()\n",
    "\n",
    "            if ref_reward == 0:\n",
    "                normalized_reward = 0.0\n",
    "            else:\n",
    "                normalized_reward = gen_reward /ref_reward  # or: gen_reward - ref_reward\n",
    "\n",
    "        rewards.append(normalized_reward)\n",
    "\n",
    "    return np.mean(rewards), np.std(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2abfe840-4eff-4f11-a2c9-e9e814442e30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean normalized reward: -0.0335 ± 0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean, std = evaluate_normalized_reward_score(\n",
    "    model, rw_model, rw_tokenizer, dataset[\"test\"], tokenizer, num_samples=100\n",
    ")\n",
    "print(f\"Mean normalized reward: {mean:.4f} ± {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41b08e-8d0a-41df-9787-558179e2106e",
   "metadata": {},
   "source": [
    "### Define PPO Configuration and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa05680e-2b2c-42ab-90f6-35dc9f998519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4ee7ba-0b21-46bf-b001-a44f2aa7a16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",    \n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=tokenizer, \n",
    "                         dataset=dataset[\"train\"], \n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e28a4f7-bfdd-4667-9555-34c5e66ad33f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -68.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "1it [00:05,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -68.79667663574219\n",
      "ppo/returns/mean: 3.9242851734161377\n",
      "ppo/policy/advantages_mean: -1.270785610074654e-08\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -60.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "2it [00:10,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -60.34709167480469\n",
      "ppo/returns/mean: 3.8933277130126953\n",
      "ppo/policy/advantages_mean: 9.148468649300412e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -69.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "3it [00:14,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -69.88340759277344\n",
      "ppo/returns/mean: 4.242911338806152\n",
      "ppo/policy/advantages_mean: -4.601772740642218e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -72.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "4it [00:20,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -72.64474487304688\n",
      "ppo/returns/mean: 4.386337757110596\n",
      "ppo/policy/advantages_mean: 2.00761807178651e-08\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -65.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "5it [00:24,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -65.13738250732422\n",
      "ppo/returns/mean: 4.216862678527832\n",
      "ppo/policy/advantages_mean: 1.7648416061888383e-08\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -60.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "6it [00:29,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -60.092926025390625\n",
      "ppo/returns/mean: 4.051173210144043\n",
      "ppo/policy/advantages_mean: -7.311508287699553e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -69.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "7it [00:35,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -69.19317626953125\n",
      "ppo/returns/mean: 4.194902420043945\n",
      "ppo/policy/advantages_mean: -2.8518353900608417e-08\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -70.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "8it [00:39,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -70.78736114501953\n",
      "ppo/returns/mean: 4.426096439361572\n",
      "ppo/policy/advantages_mean: -9.977169312946899e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -71.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "9it [00:44,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -71.92752075195312\n",
      "ppo/returns/mean: 4.503462314605713\n",
      "ppo/policy/advantages_mean: 9.902186626220555e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -68.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "10it [00:49,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -68.01797485351562\n",
      "ppo/returns/mean: 4.629611015319824\n",
      "ppo/policy/advantages_mean: 7.552273473265814e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -66.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "11it [00:54,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -66.93280029296875\n",
      "ppo/returns/mean: 4.590643882751465\n",
      "ppo/policy/advantages_mean: 2.4668612041978122e-08\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv-summarize/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -71.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "12it [00:59,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -71.49988555908203\n",
      "ppo/returns/mean: 4.566149711608887\n",
      "ppo/policy/advantages_mean: -5.170743833105007e-09\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import sigmoid\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_min_length = 100\n",
    "output_max_length = 512\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "max_ppo_steps = 12\n",
    "\n",
    "# Make sure reward_model and reward_tokenizer are loaded\n",
    "# e.g.\n",
    "# reward_model = AutoModelForSequenceClassification.from_pretrained(...).to(\"cuda\")\n",
    "# reward_tokenizer = AutoTokenizer.from_pretrained(...)\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    batch[\"response\"] = [tokenizer.decode(r, skip_special_tokens=True) for r in summary_tensors]\n",
    "\n",
    "    # --- Compute rewards using reward model ---\n",
    "    reward_texts = [q + \"\\n\" + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "\n",
    "    reward_tensors = []\n",
    "    for text in reward_texts:\n",
    "        inputs = rw_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            logits = rw_model(**inputs).logits\n",
    "            reward = sigmoid(logits).item()\n",
    "        reward_tensors.append(torch.tensor(reward))\n",
    "\n",
    "    # --- PPO update ---\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1ef94-85df-49ae-b8ce-721e45151e4b",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8265f8bb-c1c5-411a-840c-df09216bdb75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ppo_model/tokenizer_config.json',\n",
       " 'ppo_model/special_tokens_map.json',\n",
       " 'ppo_model/vocab.json',\n",
       " 'ppo_model/merges.txt',\n",
       " 'ppo_model/added_tokens.json',\n",
       " 'ppo_model/tokenizer.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_trainer.model.save_pretrained(\"ppo_model\")\n",
    "ppo_trainer.tokenizer.save_pretrained(\"ppo_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d32814-32d6-4ffb-bbf9-a062433f09a1",
   "metadata": {},
   "source": [
    "### Evaluate Reward Score after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9c1a58-5237-4c77-8ee5-50de1ed36bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.25533432024531066), np.float64(0.13090977081572575))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_reward_score(ppo_model, \n",
    "                      rw_model, \n",
    "                      rw_tokenizer, \n",
    "                      dataset['test'], \n",
    "                      tokenizer, \n",
    "                      num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96213dbb-a01c-4ab8-b43f-b7b3c1081107",
   "metadata": {},
   "source": [
    "### Compare Summary Generated Before and After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "842872b6-0149-4a03-9f47-a7c1b462104a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_model device: cuda:0\n",
      "ppo_model device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"ref_model device:\", next(ref_model.parameters()).device)\n",
    "print(\"ppo_model device:\", next(ppo_model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8989f4f7-9274-480f-93b8-f4c11023ac8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppo_model = ppo_model.to(\"cuda\")\n",
    "ref_model = ref_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f9cc96e-a6f1-4d04-bd37-e98dc25215cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_2086/3195855357.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor_ref = torch.tensor(prompt_tensors[i], device=device_ref).unsqueeze(0)\n",
      "/tmp/ipykernel_2086/3195855357.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor_ppo = torch.tensor(prompt_tensors[i], device=device_ppo).unsqueeze(0)\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][:batch_size]\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "# Get devices for each model\n",
    "device_ref = next(ref_model.parameters()).device\n",
    "device_ppo = next(ppo_model.parameters()).device\n",
    "device_reward = next(rw_model.parameters()).device\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Generate responses from reference model and PPO model\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "\n",
    "    input_tensor_ref = torch.tensor(prompt_tensors[i], device=device_ref).unsqueeze(0)\n",
    "    input_tensor_ppo = torch.tensor(prompt_tensors[i], device=device_ppo).unsqueeze(0)\n",
    "\n",
    "    summary_ref = ref_model.generate(input_ids=input_tensor_ref, **generation_kwargs).squeeze()[-gen_len:]\n",
    "    summary_ppo = ppo_model.generate(input_ids=input_tensor_ppo, **generation_kwargs).squeeze()[-gen_len:]\n",
    "\n",
    "    summary_tensors_ref.append(summary_ref)\n",
    "    summary_tensors.append(summary_ppo)\n",
    "\n",
    "# Decode responses\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(s.cpu(), skip_special_tokens=True) for s in summary_tensors_ref]\n",
    "compare_results[\"response_after\"]  = [tokenizer.decode(s.cpu(), skip_special_tokens=True) for s in summary_tensors]\n",
    "\n",
    "# Compute reward scores using your reward model\n",
    "texts_before = [q + \"\\n\" + r for q, r in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "texts_after  = [q + \"\\n\" + r for q, r in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "\n",
    "def get_rewards(texts):\n",
    "    rewards = []\n",
    "    for text in texts:\n",
    "        inputs = rw_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device_reward)\n",
    "        with torch.no_grad():\n",
    "            logits = rw_model(**inputs).logits\n",
    "            reward = sigmoid(logits).item()\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "compare_results[\"reward_before\"] = get_rewards(texts_before)\n",
    "compare_results[\"reward_after\"] = get_rewards(texts_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9cb5995-1496-48ef-ba7c-32d222cef9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SUBREDDIT: r/relationships\\nTITLE: The girl [26 F] I [22 M] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend [~30? M].\\nPOST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue.\\n\\nI saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time.\\n\\nI don't mind that she hangs out with friends, and I know it's pretty early in the relationship, but am I wrong to be a little annoyed that she didn't respond until 24 hours after my first text?\\nTL;DR: \\n\\n for some reason, ok, I texted her after I got a lot of texts late, she didn't respond even had I called on my final date. Thoughts?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_results[\"response_before\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f457dc1-19bf-4daa-9142-a074356a1ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SUBREDDIT: r/relationships\\nTITLE: The girl [26 F] I [22 M] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend [~30? M].\\nPOST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue.\\n\\nI saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time.\\n\\nI don't mind that she hangs out with friends, and I know it's pretty early in the relationship, but am I wrong to be a little annoyed that she didn't respond until 24 hours after my first text?\\nTL;DR: \\n\\n I saw her hanging out with a girl that she wasn't at. Now read this.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_results[\"response_after\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f48a0a-c7cf-4403-a977-54661601e9d8",
   "metadata": {},
   "source": [
    "### Let a More Intelligent Model (GPT-4o) be the Judge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1169488f-9f14-471f-b366-c1fcf2823cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#client = OpenAI(api_key='your api key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46799111-df6f-4269-9431-6d96b9e58723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gpt4_judge(query, response_a, response_b, client, model=\"gpt-4o\"):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful and fair assistant. You will be shown a prompt and two responses. \"\n",
    "        \"Your job is to judge which response is better, based only on helpfulness, accuracy, and coherence.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"### Summarize this Reddit Post:\n",
    "{query}\n",
    "\n",
    "### Response A:\n",
    "{response_a}\n",
    "\n",
    "### Response B:\n",
    "{response_b}\n",
    "\n",
    "Which response is better? Reply with 'A' or 'B' and explain briefly why.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89aea85d-c94e-4223-a2b8-ac569292614d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response A is better. Although it is not perfectly coherent, it attempts to summarize the situation by mentioning the lack of response and the timing of the texts. Response B, on the other hand, is incoherent and does not provide a meaningful summary or address the main points of the original post.\n",
      "\n",
      "-----------\n",
      "B\n",
      "\n",
      "Response B is better because it attempts to summarize the original post, even though it is not entirely coherent. It captures the essence of the situation: the discovery of the wife's affair and the husband's current dilemma. Response A, on the other hand, introduces unrelated and confusing information about an ex-husband, which is not relevant to the original post.\n",
      "\n",
      "-----------\n",
      "B is the better response. It accurately reflects the original post's concern about whether not engaging in sexual activities is a bad thing, without introducing unrelated or unclear elements like \"proselydering relationship\" found in A. Response B is more coherent and directly addresses the poster's question.\n",
      "\n",
      "-----------\n",
      "B\n",
      "\n",
      "Response B is better because it attempts to summarize the main issue of the original post, which is the awkwardness and tension after the prom-posal and the consideration of asking her out to ease the situation. Although the summary is not perfectly clear, it captures the essence of the dilemma more accurately than Response A, which is incoherent and does not effectively convey the main points of the original post.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are not perfect, Response B is slightly more coherent and relevant to the original post. It mentions the concept of a \"redditor\" and hints at the situation described in the post, whereas Response A is confusing and does not accurately reflect the content or context of the original post.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are not perfect, Response B attempts to provide a TL;DR that is somewhat related to the original post, mentioning the vacuum and the act of emptying it. Response A, on the other hand, includes irrelevant and incoherent text that does not relate to the original post or provide a summary.\n",
      "\n",
      "-----------\n",
      "Response A is better. It maintains coherence and stays on topic, summarizing the original post accurately. Response B, on the other hand, is incoherent and does not provide a relevant or accurate summary of the post.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While neither response provides a coherent or accurate summary of the original post, Response B at least attempts to address the issue of the boyfriend's gaming habits and the poster's concerns about their relationship. Response A, on the other hand, includes irrelevant and incoherent content that does not relate to the original post's context.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are not perfect, Response B attempts to summarize the situation by mentioning the key elements: the fake game, the threat to involve the police, and the issue with feedback. Response A, on the other hand, introduces unrelated and confusing elements like \"Euronymous\" and \"Xbox,\" which are not present in the original post and do not contribute to a coherent summary.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are incomplete and do not provide a coherent summary, Response B at least attempts to capture the essence of the original post by mentioning the inability to cook and the concern about impressing someone. Response A, on the other hand, is incoherent and introduces irrelevant and confusing phrases that do not relate to the original post.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are incomplete and do not provide a proper summary, Response B at least attempts to address the situation by posing a relevant question about preventing the breakup and improving the relationship. This aligns more closely with the original post's context of seeking advice and expressing confusion about the breakup. Response A, on the other hand, ends with a question that is not coherent or relevant to the original post.\n",
      "\n",
      "-----------\n",
      "B is better. \n",
      "\n",
      "Response B provides a more coherent and relevant summary of the original post. It captures the essence of the poster's conflict between the issues in the relationship and their feelings, as well as the practical considerations of breaking up. Response A, on the other hand, is disjointed and includes irrelevant or unclear phrases, making it less helpful and accurate.\n",
      "\n",
      "-----------\n",
      "A\n",
      "\n",
      "Response A is better because it attempts to engage with the original post by reflecting on personal feelings related to cheating and hiding things from the past. Although it is not a perfect summary, it is more relevant to the topic of the original post than Response B, which is incoherent and does not provide any meaningful engagement with the content of the post.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are not perfect, Response B attempts to engage with the theme of the original post by asking questions related to trust and judgment, which are relevant to the topic of confronting insecurities. Response A, on the other hand, is incoherent and does not relate to the original post's content or theme.\n",
      "\n",
      "-----------\n",
      "A\n",
      "\n",
      "Response A is better because it attempts to summarize the situation described in the Reddit post, even though it includes some irrelevant and confusing information at the end. Response B, on the other hand, includes completely unrelated and nonsensical information about a \"license plate number\" and a \"squirrely young cat,\" which does not pertain to the original post at all. Therefore, Response A is more coherent and relevant to the original content.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses are incomplete and do not provide a proper summary, Response B attempts to address the issue of not wearing glasses and its impact on the situation, which is more relevant to the original post. Response A, on the other hand, introduces unrelated and confusing information about a bridesmaid and does not address the main issue of the post.\n",
      "\n",
      "-----------\n",
      "A\n",
      "\n",
      "Response A is better because it attempts to provide a summary, albeit a very unclear one. However, it is slightly more coherent than Response B, which contains random and unrelated text that does not make sense in the context of the original post. Neither response is particularly helpful, but A is marginally more relevant.\n",
      "\n",
      "-----------\n",
      "Response A is better. It attempts to summarize the original post by mentioning the desire for the wife to change and the non-dealbreaker nature of her current personality. Although the summary is not perfect, it is more coherent and relevant to the original post than Response B, which is confusing and does not accurately reflect the content of the post.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While neither response provides a coherent or accurate summary of the original post, Response B is slightly more relevant as it mentions the age and educational status of the person, which could be related to the process of becoming a doctor. Response A, on the other hand, introduces unrelated and unclear information about working to the top and bike rides, which does not connect to the original post's content.\n",
      "\n",
      "-----------\n",
      "Response B is better. \n",
      "\n",
      "While both responses attempt to summarize the original post, Response B provides a more coherent and relevant TL;DR by mentioning the key issue of the Blue Screen of Death (BSOD) and the user's consideration of Windows 8. Response A, on the other hand, is incoherent and does not accurately capture the main points of the original post.\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "judgments = []\n",
    "\n",
    "for q, r_before, r_after in zip(compare_results[\"query\"],\n",
    "                                compare_results[\"response_before\"],\n",
    "                                compare_results[\"response_after\"]):\n",
    "    \n",
    "    result = gpt4_judge(q, r_before, r_after, client)\n",
    "    judgments.append(result)\n",
    "    print(result)\n",
    "    print(\"\\n-----------\")# optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "872cef33-50e8-4b22-a89f-f2fd3bd289a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_preference_from_text(judgment):\n",
    "    judgment = judgment.lower()\n",
    "    if \"response a is better\" in judgment:\n",
    "        return \"A\"\n",
    "    elif \"response b is better\" in judgment:\n",
    "        return \"B\"\n",
    "    else:\n",
    "        return \"Neither\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72ea00e4-8223-490f-962c-c3153a1649ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o preferred PPO model in 12 out of 20 comparisons.\n"
     ]
    }
   ],
   "source": [
    "preferred = [extract_preference_from_text(j) for j in judgments[:100]]\n",
    "\n",
    "ppo_wins = preferred.count(\"B\")\n",
    "print(f\"GPT-4o preferred PPO model in {ppo_wins} out of {len(preferred)} comparisons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ed0fa13-cab8-4031-b199-0d95f724dd05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'B': 12, 'A': 6, 'Neither': 2})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(preferred)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
